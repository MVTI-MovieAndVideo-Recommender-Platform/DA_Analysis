{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhqrHwHOvW6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2066c370-6ed0-4163-e318-f5f128fcc652"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.9.0\n",
            "  Downloading tensorflow-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.6.3)\n",
            "Collecting flatbuffers<2,>=1.12 (from tensorflow==2.9.0)\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.9.0)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.64.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (3.9.0)\n",
            "Collecting keras<2.10.0,>=2.9.0rc0 (from tensorflow==2.9.0)\n",
            "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-preprocessing>=1.1.1 (from tensorflow==2.9.0)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (24.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.16.0)\n",
            "Collecting tensorboard<2.10,>=2.9 (from tensorflow==2.9.0)\n",
            "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m112.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (0.37.0)\n",
            "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0 (from tensorflow==2.9.0)\n",
            "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.14.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.9.0) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.27.0)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.10,>=2.9->tensorflow==2.9.0)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.6)\n",
            "Collecting protobuf>=3.9.2 (from tensorflow==2.9.0)\n",
            "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.31.0)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.0)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.0)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.2.2)\n",
            "Installing collected packages: tensorboard-plugin-wit, keras, flatbuffers, tensorflow-estimator, tensorboard-data-server, protobuf, keras-preprocessing, gast, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 24.3.25\n",
            "    Uninstalling flatbuffers-24.3.25:\n",
            "      Successfully uninstalled flatbuffers-24.3.25\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.15.0\n",
            "    Uninstalling tensorflow-estimator-2.15.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.15.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.4\n",
            "    Uninstalling gast-0.5.4:\n",
            "      Successfully uninstalled gast-0.5.4\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.0\n",
            "    Uninstalling google-auth-oauthlib-1.2.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tensorflow-datasets 4.9.6 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 3.19.6 which is incompatible.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed flatbuffers-1.12 gast-0.4.0 google-auth-oauthlib-0.4.6 keras-2.9.0 keras-preprocessing-1.1.2 protobuf-3.19.6 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.0 tensorflow-estimator-2.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "22210e737725458f8da66d45f5ad2b92"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-text==2.9.0\n",
            "  Downloading tensorflow_text-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/4.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/4.6 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/4.6 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/4.6 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m3.9/4.6 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text==2.9.0) (0.16.1)\n",
            "Requirement already satisfied: tensorflow<2.10,>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text==2.9.0) (2.9.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (1.12)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (1.64.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (3.9.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (2.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (1.1.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (24.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (3.19.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (2.9.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (0.37.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (2.9.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (1.14.1)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub>=0.8.0->tensorflow-text==2.9.0) (2.15.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (3.0.3)\n",
            "INFO: pip is looking at multiple versions of tf-keras to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tf-keras>=2.14.1 (from tensorflow-hub>=0.8.0->tensorflow-text==2.9.0)\n",
            "  Downloading tf_keras-2.16.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tf_keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (3.2.2)\n",
            "Installing collected packages: tf-keras, tensorflow-text\n",
            "  Attempting uninstall: tf-keras\n",
            "    Found existing installation: tf_keras 2.15.1\n",
            "    Uninstalling tf_keras-2.15.1:\n",
            "      Successfully uninstalled tf_keras-2.15.1\n",
            "Successfully installed tensorflow-text-2.9.0 tf-keras-2.15.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.9.0\n",
        "!pip install tensorflow-text==2.9.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pickle\n",
        "import joblib\n",
        "import time"
      ],
      "metadata": {
        "id": "rtSIdt0svqFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 데이터 로드"
      ],
      "metadata": {
        "id": "yb7qeO94m6cb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 왓챠피디아 크롤링 데이터\n",
        "data = pd.read_csv('/content/media_data.csv', encoding = 'utf-8')\n",
        "data.info()"
      ],
      "metadata": {
        "id": "IhW8x28xwJ6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9120fe0-e64c-4e90-9845-fe9332ad78aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 18405 entries, 0 to 18404\n",
            "Data columns (total 14 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   Title           18405 non-null  object \n",
            " 1   Runtime         18405 non-null  float64\n",
            " 2   Release Date    18405 non-null  object \n",
            " 3   Certification   15783 non-null  object \n",
            " 4   Genres          18405 non-null  object \n",
            " 5   Origin Country  18334 non-null  object \n",
            " 6   Overview        18405 non-null  object \n",
            " 7   Director        15274 non-null  object \n",
            " 8   Cast            17855 non-null  object \n",
            " 9   Providers       16331 non-null  object \n",
            " 10  Rating Value    18405 non-null  float64\n",
            " 11  Rating Count    18405 non-null  int64  \n",
            " 12  Poster URL      18405 non-null  object \n",
            " 13  Backdrop URLs   18405 non-null  object \n",
            "dtypes: float64(2), int64(1), object(11)\n",
            "memory usage: 2.0+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_2 = pd.read_csv(\"/content/media_data_2.csv\", encoding='utf-8')\n",
        "data_2.info()"
      ],
      "metadata": {
        "id": "nfm6AnsoM-Yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 컬럼(피처)만 선택해서 새로운 데이터셋 생성\n",
        "content = data[['Title', 'Genres', 'Overview', 'Rating Value', 'Rating Count']]\n",
        "\n",
        "print(content.head())\n",
        "print(content.columns)"
      ],
      "metadata": {
        "id": "nl0AT3NKwYN2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d64ae6ac-d1f3-432c-d303-a0abdf6be108"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Title             Genres  \\\n",
            "0     성냥공장 소녀           코미디, 드라마   \n",
            "1     키다리 아저씨    코미디, 애니메이션, 드라마   \n",
            "2  주윤발의 행운의 별  코미디, 로맨스, 로맨틱 코미디   \n",
            "3        부귀병단            코미디, 액션   \n",
            "4        불가사리    SF, 액션, 공포, 코미디   \n",
            "\n",
            "                                            Overview  Rating Value  \\\n",
            "0  무능력하고 무표정한 얼굴의 엄마와 계부의 생활비를 위해 매일같이 성냥공장에서 기계처...           3.8   \n",
            "1  고아이지만 언제나 밝은 주디와 그런 주디를 도와주는 후견인 '키다리 아저씨'의 사랑...           4.2   \n",
            "2  재벌가 도련님 임보생은 재산 상속권을 조건으로 육촌 동생 진옥선과의 혼인을 강요당한...           3.2   \n",
            "3  1940년대 초, 일본의 침공으로 전 중국이 혼란에 빠져있을 무렵, 중국내 일본의 ...           2.9   \n",
            "4  네바다주 사막 한 가운데 있는 작은 마을에는 20명도 안되는 주민들이 서로 도우며 ...           3.3   \n",
            "\n",
            "   Rating Count  \n",
            "0          5278  \n",
            "1          1623  \n",
            "2           184  \n",
            "3           128  \n",
            "4         61592  \n",
            "Index(['Title', 'Genres', 'Overview', 'Rating Value', 'Rating Count'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content.info()"
      ],
      "metadata": {
        "id": "jSJ-t3I3ynoP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0abe063e-101c-4fe1-ce12-e9a119e8ee1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 18405 entries, 0 to 18404\n",
            "Data columns (total 5 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Title         18405 non-null  object \n",
            " 1   Genres        18405 non-null  object \n",
            " 2   Overview      18405 non-null  object \n",
            " 3   Rating Value  18405 non-null  float64\n",
            " 4   Rating Count  18405 non-null  int64  \n",
            "dtypes: float64(1), int64(1), object(3)\n",
            "memory usage: 719.1+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MBTI 500 데이터\n",
        "mbti = pd.read_csv('/content/MBTI 500.csv')\n",
        "print(mbti.info())"
      ],
      "metadata": {
        "id": "D2lWKUsFn_qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. LaBSE 모델 로드 및 임베딩"
      ],
      "metadata": {
        "id": "weT2OJcIo95_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 사전 학습된 LaBSE 모델을 사용하여 텍스트 데이터를 고차원 벡터로 변환합니다.\n",
        "- 이 임베딩 모델은 다국어 문장 임베딩을 위한 모델로, 15개 언어만 사용하도록 하여 경량화된 모델입니다.\n",
        "- 고차원 벡터로 변환된 텍스트는 유사도 계산에 사용됩니다."
      ],
      "metadata": {
        "id": "hY4zanQTpCDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LaBSE 모델 로드\n",
        "encoder = hub.KerasLayer(\"https://tfhub.dev/jeongukjae/smaller_LaBSE_15lang/1\")\n",
        "preprocessor = hub.KerasLayer(\"https://tfhub.dev/jeongukjae/smaller_LaBSE_15lang_preprocess/1\")\n",
        "\n",
        "# 텍스트를 고차원 벡터로 인코딩하는 모델 구성\n",
        "def build_embedding_model():\n",
        "    sentences = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"sentences\")\n",
        "    encoder_inputs = preprocessor(sentences)\n",
        "    sentence_representation = encoder(encoder_inputs)[\"pooled_output\"]\n",
        "    normalized_sentence_representation = tf.nn.l2_normalize(sentence_representation, axis=-1)  # for cosine similarity\n",
        "    return tf.keras.Model(sentences, normalized_sentence_representation)\n",
        "\n",
        "embedding_model = build_embedding_model()\n",
        "embedding_model.summary()"
      ],
      "metadata": {
        "id": "2b7h3viqyqJz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad935fee-3777-4821-b693-0572be7ee14b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " sentences (InputLayer)         [(None,)]            0           []                               \n",
            "                                                                                                  \n",
            " keras_layer_1 (KerasLayer)     {'input_mask': (Non  0           ['sentences[0][0]']              \n",
            "                                e, 128),                                                          \n",
            "                                 'input_type_ids':                                                \n",
            "                                (None, 128),                                                      \n",
            "                                 'input_word_ids':                                                \n",
            "                                (None, 128)}                                                      \n",
            "                                                                                                  \n",
            " keras_layer (KerasLayer)       {'pooled_output': (  219171840   ['keras_layer_1[0][0]',          \n",
            "                                None, 768),                       'keras_layer_1[0][1]',          \n",
            "                                 'encoder_outputs':               'keras_layer_1[0][2]']          \n",
            "                                 [(None, 128, 768),                                               \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768)],                                               \n",
            "                                 'sequence_output':                                               \n",
            "                                 (None, 128, 768)}                                                \n",
            "                                                                                                  \n",
            " tf.math.l2_normalize (TFOpLamb  (None, 768)         0           ['keras_layer[0][12]']           \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 219,171,840\n",
            "Trainable params: 0\n",
            "Non-trainable params: 219,171,840\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트 임베딩 함수\n",
        "def embed_texts(texts, batch_size=32):\n",
        "    embeddings = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        batch_tensors = tf.constant(batch_texts)\n",
        "        batch_embeddings = embedding_model(batch_tensors).numpy()\n",
        "        embeddings.extend(batch_embeddings)\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "Ejzb7I9UytjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MBTI 임베딩\n",
        "mbti_posts = mbti.groupby('type')['posts'].apply(lambda posts: ' '.join(posts)).tolist()\n",
        "mbti_embeddings = embed_texts(mbti_posts)\n",
        "mbti_index = mbti.groupby('type').groups.keys()\n",
        "mbti_embeddings_dict = dict(zip(mbti_index, mbti_embeddings))\n",
        "\n",
        "# 콘텐츠 줄거리 임베딩\n",
        "contents_texts = content['Overview'].tolist()\n",
        "contents_embeddings = embed_texts(contents_texts)\n",
        "contents_index = content['Title'].tolist()\n",
        "contents_embeddings_dict = dict(zip(contents_index, contents_embeddings))"
      ],
      "metadata": {
        "id": "lKClhJGXyzkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 콘텐츠 임베딩 파일 저장 및 로드\n",
        "with open('contents_embeddings_dict.pkl', 'wb') as f:\n",
        "    pickle.dump(contents_embeddings_dict, f)"
      ],
      "metadata": {
        "id": "uKMs2NAdy009"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MBTI 임베딩 파일 저장 및 로드\n",
        "with open('mbti_embeddings_dict.pkl', 'rb') as f:\n",
        "    mbti_embeddings_dict = pickle.load(f)"
      ],
      "metadata": {
        "id": "4u8SL6utz1hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 콘텐츠 임베딩 로드\n",
        "with open('contents_embeddings_dict_0612.pkl', 'rb') as f:\n",
        "    contents_embeddings_dict = pickle.load(f)"
      ],
      "metadata": {
        "id": "dBzWXgtd4qws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 콘텐츠 장르 원-핫 인코딩 / 데이터 표준화 / 콘텐츠 평점 개수 정규화"
      ],
      "metadata": {
        "id": "2wpK_lFop7Uz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 콘텐츠 장르를 원-핫 인코딩하여 장르 정보를 벡터화합니다.\n",
        "- 원-핫 인코딩된 장르 벡터와 평점 데이터를 결합하여 학습 데이터를 생성합니다.\n",
        "- 데이터 표준화를 통해 모델 학습을 용이하게 합니다.\n",
        "- 대중성 점수로 사용하기 위해 콘텐츠 평점 개수를 정규화합니다."
      ],
      "metadata": {
        "id": "MWlFEohSqGIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 콘텐츠 장르 원-핫 인코딩\n",
        "genres = content['Genres'].str.get_dummies(sep=', ')\n",
        "\n",
        "# 평점 데이터 결합\n",
        "cbf_model_input = np.hstack([genres.values, content['Rating Value'].values.reshape(-1, 1)])\n",
        "\n",
        "# 데이터 표준화\n",
        "cbf_scaler = StandardScaler()\n",
        "cbf_model_input_scaled = cbf_scaler.fit_transform(cbf_model_input)"
      ],
      "metadata": {
        "id": "H1wlVo92zGkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 콘텐츠 평점 개수를 정규화하는 함수\n",
        "def normalize_popularity_score(content):\n",
        "    scaler = MinMaxScaler()\n",
        "    content['Normalized Popularity Score'] = scaler.fit_transform(content[['Rating Count']])\n",
        "    return content\n",
        "\n",
        "content = normalize_popularity_score(content)"
      ],
      "metadata": {
        "id": "3YqyCGkXEHgc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c553841-e8c5-4cca-f55f-ba6e6d050e51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-e69302caf052>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  content['Normalized Popularity Score'] = scaler.fit_transform(content[['Rating Count']])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. 모델 학습 및 최적화"
      ],
      "metadata": {
        "id": "1tyFtBWPrRTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 학습 데이터와 평가 데이터로 분리합니다.\n",
        "- GBM(Gradient Boosting Machine)을 사용하여 모델을 학습합니다.\n",
        "- Grid Search를 통해 최적의 하이퍼파라미터를 탐색하고, 최적의 모델을 선정합니다."
      ],
      "metadata": {
        "id": "NRVIyyTfrUQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 데이터와 평가 데이터 분리\n",
        "X_train, X_val, y_train, y_val = train_test_split(cbf_model_input_scaled, content['Rating Value'].values, test_size=0.2, random_state=42)\n",
        "\n",
        "# 하이퍼파라미터 그리드 설정\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200, 300, 500],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'max_depth': [3, 4, 5, 6, 7],\n",
        "    'subsample': [0.6, 0.8, 1.0]\n",
        "}\n",
        "\n",
        "# 그리드 서치 설정\n",
        "gbm = GradientBoostingRegressor(random_state=42)\n",
        "grid_search = GridSearchCV(estimator=gbm, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=1, verbose=2)\n",
        "\n",
        "# 그리드 서치 수행\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 최적의 모델 출력\n",
        "best_gbm = grid_search.best_estimator_\n",
        "print(f'Best GBM Model: {best_gbm}')\n",
        "\n",
        "# 검증 데이터로 모델 성능 평가\n",
        "y_pred = best_gbm.predict(X_val)\n",
        "mse = mean_squared_error(y_val, y_pred)\n",
        "print(f'Validation MSE: {mse}')\n",
        "\n",
        "# 모델 저장\n",
        "joblib.dump(best_gbm, 'best_gbm_model_final.pkl')"
      ],
      "metadata": {
        "id": "larJZXFZzQ0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최적의 모델 로드\n",
        "best_gbm = joblib.load('best_gbm_model_final_0612.pkl')"
      ],
      "metadata": {
        "id": "Kt8cQcHRzZ9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. 콘텐츠 추천 시스템 구현"
      ],
      "metadata": {
        "id": "RUI9daX0roFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **유사도 계산 함수**: 두 임베딩 벡터 간의 코사인 유사도를 계산합니다.\n",
        "- **MBTI 기반 추천**: 사용자의 MBTI 임베딩과 콘텐츠 임베딩 간의 유사도를 계산하여 콘텐츠를 추천합니다.\n",
        "- **선호 콘텐츠 기반 추천**: 사용자가 선호하는 콘텐츠와 유사한 콘텐츠를 추천합니다."
      ],
      "metadata": {
        "id": "a4tBAQYerq9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 콘텐츠와 MBTI 유형 간의 유사도 계산 함수\n",
        "def calculate_similarity(content_embedding, user_embedding):\n",
        "    from numpy import dot\n",
        "    from numpy.linalg import norm\n",
        "    return dot(content_embedding, user_embedding) / (norm(content_embedding) * norm(user_embedding))\n",
        "\n",
        "# 1. MBTI 임베딩 기반 추천\n",
        "def recommend_contents_by_mbti(user_embedding, content_embeddings_dict, top_n=100):\n",
        "    recommended_contents = []\n",
        "    for content_id, embedding in content_embeddings_dict.items():\n",
        "        similarity = calculate_similarity(embedding, user_embedding)\n",
        "        recommended_contents.append((content_id, similarity))\n",
        "    recommended_contents = sorted(recommended_contents, key=lambda x: x[1], reverse=True)\n",
        "    return recommended_contents[:top_n]\n",
        "\n",
        "# 2. 선호 콘텐츠 유사도 기반 추천\n",
        "def recommend_similar_contents(preferred_contents, content_embeddings_dict, top_n=100):\n",
        "    similar_contents = {}\n",
        "    for content in preferred_contents:\n",
        "        content_embedding = content_embeddings_dict[content]\n",
        "        for other_content, embedding in content_embeddings_dict.items():\n",
        "            if other_content not in preferred_contents:\n",
        "                similarity = calculate_similarity(embedding, content_embedding)\n",
        "                if other_content not in similar_contents:\n",
        "                    similar_contents[other_content] = 0\n",
        "                similar_contents[other_content] += similarity\n",
        "    similar_contents = sorted(similar_contents.items(), key=lambda x: x[1], reverse=True)\n",
        "    return similar_contents[:top_n]"
      ],
      "metadata": {
        "id": "hAh5vYVSzcZr"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **결합 추천 시스템**: MBTI 유사도 점수, 선호 콘텐츠 유사도 점수, 모델 점수, 대중성 점수에 각각 가중치를 부여하여 콘텐츠를 추천합니다."
      ],
      "metadata": {
        "id": "h5oiKzhus8vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 결합 추천 시스템\n",
        "def recommend_contents_combined(user_embedding, preferred_contents, content_embeddings_dict, model, content, cbf_model_input_scaled, top_n=20):\n",
        "    weight_mbti = 0.3\n",
        "    weight_similar = 0.4\n",
        "    weight_model = 0.1\n",
        "    weight_popularity = 0.2\n",
        "\n",
        "    mbti_recommendations = recommend_contents_by_mbti(user_embedding, content_embeddings_dict, top_n=100)\n",
        "    similar_content_recommendations = recommend_similar_contents(preferred_contents, content_embeddings_dict, top_n=100)\n",
        "\n",
        "    combined_recommendations = {}\n",
        "\n",
        "    # MBTI 유사도 점수 보정\n",
        "    mbti_scores = [score for _, score in mbti_recommendations]\n",
        "    mbti_std = np.std(mbti_scores)\n",
        "    mbti_scores_normalized = [score / mbti_std for score in mbti_scores]\n",
        "\n",
        "    for (content_id, _), score in zip(mbti_recommendations, mbti_scores_normalized):\n",
        "        if content_id not in combined_recommendations:\n",
        "            combined_recommendations[content_id] = 0\n",
        "        combined_recommendations[content_id] += weight_mbti * score\n",
        "\n",
        "    # 선호 콘텐츠 유사도 점수 보정\n",
        "    similar_scores = [score for _, score in similar_content_recommendations]\n",
        "    similar_std = np.std(similar_scores)\n",
        "    similar_scores_normalized = [score / similar_std for score in similar_scores]\n",
        "\n",
        "    for (content_id, _), score in zip(similar_content_recommendations, similar_scores_normalized):\n",
        "        if content_id not in combined_recommendations:\n",
        "            combined_recommendations[content_id] = 0\n",
        "        combined_recommendations[content_id] += weight_similar * score\n",
        "\n",
        "    # 모델 점수 보정\n",
        "    content_indices = [content.index[content['Title'] == content_id][0] for content_id in combined_recommendations.keys()]\n",
        "    if content_indices:\n",
        "        content_features = cbf_model_input_scaled[content_indices]\n",
        "        model_scores = model.predict(content_features).flatten()\n",
        "        model_std = np.std(model_scores)\n",
        "        model_scores_normalized = [score / model_std for score in model_scores]\n",
        "\n",
        "        for content_id, model_score in zip(combined_recommendations.keys(), model_scores_normalized):\n",
        "            combined_recommendations[content_id] += weight_model * model_score\n",
        "\n",
        "    # 대중성 점수 보정\n",
        "    popularity_scores = [content.loc[content['Title'] == content_id, 'Normalized Popularity Score'].values[0] for content_id in combined_recommendations.keys()]\n",
        "    popularity_std = np.std(popularity_scores)\n",
        "    popularity_scores_normalized = [score / popularity_std for score in popularity_scores]\n",
        "\n",
        "    for content_id, popularity_score in zip(combined_recommendations.keys(), popularity_scores_normalized):\n",
        "        combined_recommendations[content_id] += weight_popularity * popularity_score\n",
        "\n",
        "    filtered_recommendations = {content_id: score for content_id, score in combined_recommendations.items()\n",
        "                                if content.loc[content['Title'] == content_id, 'Rating Value'].values[0] >= 3.5}\n",
        "\n",
        "    final_recommendations = sorted(filtered_recommendations.items(), key=lambda x: x[1], reverse=True)\n",
        "    return [content_id for content_id, _ in final_recommendations[:top_n]]"
      ],
      "metadata": {
        "id": "ak2mPV0ZznU8"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 재추천 시스템: 첫 번째 추천 목록에서 사용자가 마음에 드는(선택한) 콘텐츠를 기반으로 이전에 추천된 콘텐츠와 처음에 선호 콘텐츠로 선택한 콘텐츠를 제외하고 재추천을 수행합니다."
      ],
      "metadata": {
        "id": "sb0SzJyKuLcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 재추천 시스템\n",
        "def recommend_contents_combined_excluding(user_embedding, preferred_contents, previous_recommendations, initial_preferred_contents, content_embeddings_dict, model, content, cbf_model_input_scaled, top_n=20):\n",
        "    # 가중치 설정\n",
        "    weight_mbti = 0.3\n",
        "    weight_similar = 0.4\n",
        "    weight_model = 0.1\n",
        "    weight_popularity = 0.2\n",
        "\n",
        "    # 초기 선호 콘텐츠와 현재 선호 콘텐츠 합치기\n",
        "    all_preferred_contents = list(set(preferred_contents + initial_preferred_contents))\n",
        "\n",
        "    # MBTI와 유사 콘텐츠 추천 상위 100개 가져오기\n",
        "    mbti_recommendations = recommend_contents_by_mbti(user_embedding, content_embeddings_dict, top_n=100)\n",
        "    similar_content_recommendations = recommend_similar_contents(all_preferred_contents, content_embeddings_dict, top_n=100)\n",
        "\n",
        "    # 추천 콘텐츠를 저장할 딕셔너리 초기화\n",
        "    combined_recommendations = {}\n",
        "\n",
        "    # 모든 추천 콘텐츠 ID 집합 생성 & 중복 제거\n",
        "    all_recommendations = set([content_id for content_id, _ in mbti_recommendations] +\n",
        "                              [content_id for content_id, _ in similar_content_recommendations])\n",
        "\n",
        "    # 이전에 추천된 콘텐츠와 사용자의 초기 선호 콘텐츠 필터링\n",
        "    filtered_content_ids = {content_id for content_id in all_recommendations\n",
        "                            if content_id not in previous_recommendations and\n",
        "                            content_id not in all_preferred_contents}\n",
        "\n",
        "    # 필터링된 콘텐츠의 추천 점수 초기화\n",
        "    for content_id in filtered_content_ids:\n",
        "        combined_recommendations[content_id] = 0\n",
        "\n",
        "    # MBTI 유사도 점수 보정\n",
        "    mbti_scores = [score for content_id, score in mbti_recommendations if content_id in filtered_content_ids]\n",
        "    mbti_std = np.std(mbti_scores) # std\n",
        "    mbti_scores_normalized = [score / mbti_std for score in mbti_scores]\n",
        "\n",
        "    # 필터링된 콘텐츠의 MBTI 점수 가중치 적용\n",
        "    for (content_id, _), score in zip(mbti_recommendations, mbti_scores_normalized):\n",
        "        if content_id in filtered_content_ids:\n",
        "            combined_recommendations[content_id] += weight_mbti * score\n",
        "\n",
        "    # 선호 콘텐츠 유사도 점수 보정\n",
        "    similar_scores = [score for content_id, score in similar_content_recommendations if content_id in filtered_content_ids]\n",
        "    similar_std = np.std(similar_scores)\n",
        "    similar_scores_normalized = [score / similar_std for score in similar_scores]\n",
        "\n",
        "    # 필터링된 콘텐츠의 유사 콘텐츠 점수 가중치 적용\n",
        "    for (content_id, _), score in zip(similar_content_recommendations, similar_scores_normalized):\n",
        "        if content_id in filtered_content_ids:\n",
        "            combined_recommendations[content_id] += weight_similar * score\n",
        "\n",
        "    # 모델 점수 보정\n",
        "    # 필터링된 콘텐츠 인덱스\n",
        "    content_indices = [content.index[content['Title'] == content_id][0] for content_id in filtered_content_ids]\n",
        "    if content_indices:\n",
        "        # 모델에 입력할 스케일링된 피쳐\n",
        "        content_features = cbf_model_input_scaled[content_indices]\n",
        "        model_scores = model.predict(content_features).flatten()\n",
        "        model_std = np.std(model_scores)\n",
        "        model_scores_normalized = [score / model_std for score in model_scores]\n",
        "\n",
        "        # 필터링된 콘텐츠의 모델 점수 가중치 적용\n",
        "        for content_id, model_score in zip(filtered_content_ids, model_scores_normalized):\n",
        "            combined_recommendations[content_id] += weight_model * model_score\n",
        "\n",
        "    # 대중성 점수 보정\n",
        "    # 필터링된 콘텐츠의 대중성 점수\n",
        "    popularity_scores = [content.loc[content['Title'] == content_id, 'Normalized Popularity Score'].values[0] for content_id in filtered_content_ids]\n",
        "    popularity_std = np.std(popularity_scores)\n",
        "    popularity_scores_normalized = [score / popularity_std for score in popularity_scores]\n",
        "\n",
        "    # 필터링된 콘텐츠의 대중성 점수 가중치 적용\n",
        "    for content_id, popularity_score in zip(filtered_content_ids, popularity_scores_normalized):\n",
        "        combined_recommendations[content_id] += weight_popularity * popularity_score\n",
        "\n",
        "    # 평점이 3.5 이상인 콘텐츠만 필터링\n",
        "    filtered_recommendations = {content_id: score for content_id, score in combined_recommendations.items()\n",
        "                                if content.loc[content['Title'] == content_id, 'Rating Value'].values[0] >= 3.5}\n",
        "\n",
        "    # 점수 내림차순으로 정렬 -> 최종 추천 콘텐츠 ID 리턴\n",
        "    final_recommendations = sorted(filtered_recommendations.items(), key=lambda x: x[1], reverse=True)\n",
        "    return [content_id for content_id, _ in final_recommendations[:top_n]]"
      ],
      "metadata": {
        "id": "s5OZ2lPjzp3j"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용자 입력 추천 함수\n",
        "def user_input_recommendation(mbti_type, preferred_contents, content, cbf_model_input_scaled):\n",
        "    user_embedding = mbti_embeddings_dict[mbti_type]\n",
        "\n",
        "    # 최적화된 결합 추천 시스템 실행\n",
        "    recommendations = recommend_contents_combined(user_embedding, preferred_contents, contents_embeddings_dict, best_gbm, content, cbf_model_input_scaled)\n",
        "\n",
        "    return recommendations"
      ],
      "metadata": {
        "id": "333f-hH6zrAs"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. 추천 실행"
      ],
      "metadata": {
        "id": "8NcFkK1SuxMj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 예시 사용자의 입력 데이터를 사용해서 결합 추천 시스템과 재추천 시스템을 실행합니다."
      ],
      "metadata": {
        "id": "lY4c2KbwuyyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 예시 사용자 입력 데이터\n",
        "new_user_mbti = \"INTP\"\n",
        "new_user_preferred_contents = ['어벤져스: 엔드게임', '어메이징 스파이더맨']\n",
        "\n",
        "# 최적화된 결합 추천 시스템 실행\n",
        "start_time = time.time()\n",
        "optimized_recommendations = user_input_recommendation(new_user_mbti, new_user_preferred_contents, content, cbf_model_input_scaled)\n",
        "end_time = time.time()\n",
        "print(\"최적화된 결합 추천 콘텐츠 목록:\", optimized_recommendations)\n",
        "print(f\"최적화된 추천 시스템 실행 시간: {end_time - start_time:.2f} 초\")"
      ],
      "metadata": {
        "id": "MospvC1BzthL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8821b4a9-71c1-4b96-c355-47e3decda51f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최적화된 결합 추천 콘텐츠 목록: ['매트릭스', '스파이더맨: 파 프롬 홈', '어메이징 스파이더맨 2', '토르: 다크 월드', '오블리비언', '스파이더맨 3', '캡틴 아메리카: 윈터 솔져', '아이언맨 3', '스파이더맨 2', '스파이더맨', '미스 페레그린과 이상한 아이들의 집', '나비 효과', '나니아 연대기: 새벽 출정호의 항해', '종말의 세라프', '머나먼 세상속으로', '피니와 퍼브 무비: 2차원을 넘어서', '오펀: 천사의 비밀', '프린세스 스타의 모험일기', '가디언즈 오브 갤럭시 Vol. 2', '판의 미로: 오필리아와 세개의 열쇠']\n",
            "최적화된 추천 시스템 실행 시간: 1.08 초\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫 번째 추천으로부터 재추천\n",
        "liked_contents_input = input(\"첫 번째 추천 목록에서 선호하는 콘텐츠를 쉼표로 구분하여 입력하세요: \")\n",
        "liked_contents = [content.strip() for content in liked_contents_input.split(',')]\n",
        "\n",
        "previous_recommendations = optimized_recommendations\n",
        "new_preferred_contents = liked_contents  # 사용자 입력을 기반으로 새로운 선호 콘텐츠 목록 생성\n",
        "\n",
        "# 재추천 시스템 실행 시간 측정\n",
        "start_time = time.time()\n",
        "new_user_embedding = mbti_embeddings_dict[new_user_mbti]\n",
        "second_recommendations = recommend_contents_combined_excluding(new_user_embedding, new_preferred_contents, previous_recommendations, new_user_preferred_contents, contents_embeddings_dict, best_gbm, content, cbf_model_input_scaled)\n",
        "end_time = time.time()\n",
        "\n",
        "print(\"재추천 콘텐츠 목록:\", second_recommendations)\n",
        "print(f\"재추천 시스템 실행 시간: {end_time - start_time:.2f} 초\")"
      ],
      "metadata": {
        "id": "KsR3ELIHz_J9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd9e0e7e-6979-4525-fdb0-f40e512dd829"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "첫 번째 추천 목록에서 선호하는 콘텐츠를 쉼표로 구분하여 입력하세요: 매트릭스, 스파이더맨: 파 프롬 홈\n",
            "재추천 콘텐츠 목록: ['어벤져스', '메이즈 러너', '매트릭스 3: 레볼루션', '맨 인 블랙 3', '어벤져스: 에이지 오브 울트론', '터닝메카드', '더 기버: 기억전달자', '시도니아의 기사 극장판', '트랜스포머 프라임 비스트헌터: 프레데콘 라이징', '나소흑전기: 첫만남편', '장화신은 고양이: 끝내주는 모험', '드래곤볼 슈퍼', '낙원추방', '부니베어: 원시시대 대모험', '취성의 가르간티아', '아머드 사우루스', '맨 인 블랙', '늑대소년', '마법소녀를 동경해서', '바스터즈: 거친 녀석들']\n",
            "재추천 시스템 실행 시간: 1.29 초\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. MVTI 추천 시스템의 특징 및 장점"
      ],
      "metadata": {
        "id": "NhNn-BysvAcf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. 하이브리드 추천 시스템**\n",
        "- 이 추천 시스템은 다양한 방법을 결합하여 최적의 추천을 제공합니다.\n",
        "- **MBTI 기반 추천**\n",
        "  - 사용자의 MBTI 유형을 바탕으로 콘텐츠를 추천합니다. 이는 사용자의 성격 유형과 선호도에 기반하여 추천을 제공하므로, 사용자 맞춤형 추천의 정밀도를 높입니다.\n",
        "  - **Cold Start 문제 해결**: 새로운 사용자의 초기 데이터가 적어도, MBTI 기반으로 개인화된 추천을 제공할 수 있어 초기에 데이터가 부족한 상황에서도 효과적입니다.\n",
        "\n",
        "- **선호 콘텐츠 유사도 기반 추천**\n",
        "  - 사용자가 이미 선호하는 콘텐츠와 유사한 특성을 가진 콘텐츠를 추천합니다.\n",
        "  - 콘텐츠의 특성을 분석하여 사용자 취향에 맞춘 추천을 제공합니다.\n",
        "\n",
        "- **모델 기반 예측**\n",
        "  - 학습된 기계 학습 모델을 사용하여 콘텐츠의 평점을 예측하고, 이를 바탕으로 추천합니다.\n",
        "  - 객관적인 평가 지표를 통해 콘텐츠의 품질을 고려한 추천이 가능합니다.\n",
        "\n",
        "**2. 결합 추천 시스템**\n",
        "- 여러 추천 소스를 결합하여 최적의 추천을 제공하는 시스템입니다. 각 추천 소스의 결과를 조합하여 사용자에게 최적화된 추천을 제공합니다.\n",
        "- **가중치 조정**\n",
        "  - MBTI 기반 추천, 선호 콘텐츠 기반 추천, 모델 기반 추천, 대중성 점수 등을 각각의 가중치를 조정하여 결합합니다.\n",
        "  - 각 요소가 사용자의 취향을 얼마나 잘 반영하는지를 고려하여 가중치를 조절합니다.\n",
        "\n",
        "- **점수 보정 및 정규화**\n",
        "  - 각 추천 소스의 점수를 표준화하여 비교 가능하게 만듭니다.\n",
        "  - 이로 인해 특정 소스의 점수가 과도하게 영향을 미치는 것을 방지하고, 균형 잡힌 추천이 가능해집니다.\n",
        "\n",
        "**3. 고성능 텍스트 임베딩 모델 활용**\n",
        "- **LaBSE 모델 사용**\n",
        "  - LaBSE(Language-agnostic Bert Sentence Embedding) 모델을 활용하여 텍스트 데이터를 고차원 벡터로 변환합니다.\n",
        "  - 이 모델은 다국어 지원이 가능하며, 높은 정확도의 임베딩을 제공합니다.\n",
        "  - 콘텐츠 줄거리와 MBTI 유형별 텍스트를 효과적으로 벡터화하여 유사도를 계산할 수 있습니다.\n",
        "\n",
        "**4. 데이터 통합 및 활용**\n",
        "- **텍스트 임베딩과 메타데이터 통합**\n",
        "  - 콘텐츠 줄거리(텍스트 데이터) 뿐만 아니라 장르, 평점과 같은 메타데이터도 함께 사용하여 추천을 수행합니다.\n",
        "  - 단순 텍스트 유사도 기반의 추천을 넘어, 콘텐츠의 다양한 특성을 반영한 종합적인 추천을 제공합니다.\n",
        "\n",
        "**5. 개인화된 추천**\n",
        "- **사용자 성향 반영**\n",
        "  - MBTI 유형을 기반으로 사용자의 성향을 반영한 콘텐츠 추천이 가능합니다.\n",
        "- **사용자 선호 콘텐츠 반영**\n",
        "  - 사용자가 선호하는 콘텐츠를 바탕으로 유사한 콘텐츠를 추천하여, 사용자 취향에 맞춘 개인화된 추천을 제공합니다.\n",
        "\n",
        "**6. 동적 추천 시스템**\n",
        "- **재추천 기능**\n",
        "  - 이전에 추천된 콘텐츠를 제외하고 새로운 콘텐츠를 추천합니다.\n",
        "  - 사용자가 이미 추천 받은 콘텐츠를 반복적으로 추천하는 것을 방지하며, 지속적으로 새로운 콘텐츠를 발견할 수 있도록 합니다.\n",
        "\n",
        "**7. 대중성 점수 반영**\n",
        "- 콘텐츠의 대중성을 점수에 반영하여 인기 있는 콘텐츠를 추천합니다.\n",
        "- **Normalized Popularity Score**\n",
        "  - 콘텐츠의 인기도를 정규화하여 점수에 반영합니다.\n",
        "  - 인기가 많은 콘텐츠를 추천하여, 사용자들이 선호할 가능성이 높은 콘텐츠를 제공합니다.\n",
        "\n",
        "**8. 확장 가능성**\n",
        "- **다양한 데이터 소스 통합**\n",
        "  - 현재 추천 시스템은 MBTI 데이터와 콘텐츠 데이터에 기반하지만, 사용자 행동 데이터나 소셜 미디어 데이터 등의 추가적인 데이터 소스를 통합하여 더 정교한 추천 시스템으로 확장할 수 있습니다.\n",
        "\n",
        "- **다국어 지원**\n",
        "  - LaBSE 모델의 다국어 지원을 통해 다양한 언어 콘텐츠를 추천할 수 있습니다.\n",
        "  - 이는 글로벌 사용자들에게도 유용한 추천 시스템을 제공할 수 있게 합니다.\n",
        "\n",
        "**9. 결론**\n",
        "- 이 추천 시스템은 다양한 추천 기법을 결합하여 사용자 맞춤형 추천의 정밀도와 다양성을 높입니다. 고성능 임베딩 모델과 다양한 데이터 소스를 통합하여, 사용자에게 가장 적합한 콘텐츠를 제공하며, 지속적인 성능 평가와 개선을 통해 추천의 정확성을 유지합니다. 또한, 확장 가능성과 다국어 지원을 통해 글로벌 사용자에게도 효율적인 추천 시스템을 제공합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "wXwgXcAjvFBT"
      }
    }
  ]
}